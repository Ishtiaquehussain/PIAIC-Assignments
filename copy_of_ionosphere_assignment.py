# -*- coding: utf-8 -*-
"""Copy of Ionosphere Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oQmNFAyISbHH35zqZV3gmUFbMFBY8-h6

# Assignment: Ionosphere Data Problem

### Dataset Description: 

This radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere. "Good" radar returns are those showing evidence of some type of structure in the ionosphere. "Bad" returns are those that do not; their signals pass through the ionosphere.

Received signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.

### Attribute Information:

- All 34 are continuous
- The 35th attribute is either "good" or "bad" according to the definition summarized above. This is a binary classification task.

 <br><br>

<table border="1"  cellpadding="6">
	<tbody>
        <tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Data Set Characteristics:&nbsp;&nbsp;</b></p></td>
		<td><p class="normal">Multivariate</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Instances:</b></p></td>
		<td><p class="normal">351</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Area:</b></p></td>
		<td><p class="normal">Physical</p></td>
        </tr>
     </tbody>
    </table>
<table border="1" cellpadding="6">
    <tbody>
        <tr>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Attribute Characteristics:</b></p></td>
            <td><p class="normal">Integer,Real</p></td>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Number of Attributes:</b></p></td>
            <td><p class="normal">34</p></td>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Date Donated</b></p></td>
            <td><p class="normal">N/A</p></td>
        </tr>
     </tbody>
    </table>
<table border="1" cellpadding="6">	
    <tbody>
    <tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Associated Tasks:</b></p></td>
		<td><p class="normal">Classification</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Missing Values?</b></p></td>
		<td><p class="normal">N/A</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Web Hits:</b></p></td>
		<td><p class="normal">N/A</p></td>
	</tr>
    </tbody>
    </table>

### WORKFLOW :
- Load Data
- Check Missing Values ( If Exist ; Fill each record with mean of its feature ) or any usless column.
- Shuffle the data if needed.
- Standardized the Input Variables. **Hint**: Centeralized the data
- Split into 60 and 40 ratio.
- Encode labels.
- Model : 1 hidden layers including 16 unit.
- Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)
- Train the Model with Epochs (100).
- If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .
- Prediction should be > **92%**
- Evaluation Step
- Prediction

# Load Data:
[Click Here to Download DataSet](https://github.com/ramsha275/ML_Datasets/blob/main/ionosphere_data.csv)
"""



import tensorflow as tf

print(tf.__version__)

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



pd.__version__



np.__version__

from google.colab import files
uploaded = files.upload()

import io

ionosphere_data_df = pd.read_csv ( io.BytesIO(uploaded['ionosphere_data.txt']) )

ionosphere_data_df.shape

"""It's clear from the shape of the data that dataset is not a huge one. Only 351 records are available with 34 features/columns."""

ionosphere_data_df.head()



ionosphere_data_df.describe().T

for feature in ionosphere_data_df:
    print(feature)
    print(len(ionosphere_data_df[feature].unique()))



ionosphere_data_df['feature2'].unique()

ionosphere_data_df.drop(ionosphere_data_df.columns[1], inplace=True, axis=1)

ionosphere_data_df.head()

ionosphere_data_df.ndim

ionosphere_data_df.info()

'''for feature in ionosphere_data_df:
    print(feature)
    ionosphere_data_df[feature].hist()
    plt.show()'''

# df.hist()
# plt.show()

# Check summary statistics
ionosphere_data_df.describe()

"""Check Missing Values ( If Exist ; Fill each record with mean of its feature ) or any usless column."""

# Find missing values
ionosphere_data_df.isnull().sum()

ionosphere_data_df['label'] = [1 if lbl == 'g' else 0 for lbl in ionosphere_data_df['label']]



train_data = ionosphere_data_df.sample(frac= 0.6, random_state=125)
test_data = ionosphere_data_df.drop(train_data.index)

train_label = train_data.iloc[:,-1]
train_data = train_data.iloc[:,0:-1]
test_label = test_data.iloc[:,-1]
test_data = test_data.iloc[:,0:-1]

ionosphere_data_df.drop(columns= 'label', inplace = True)

train_data.head()

train_label

"""- Standardized the Input Variables. **Hint**: Centeralized the data"""

# # Normalize the data
# train_mean = train_data.mean()
# train_data -= train_mean
# train_std = train_data.std()
# train_data /= train_std
# test_data -= train_mean
# test_data /= train_std

"""- Encode labels.

- Shuffle the data if needed.
- Split into 60 and 40 ratio.
"""

# Now sample the dataframe

train_data.shape

test_data.shape

train_label.shape

test_label.shape

train_label.sum()

len(train_label)

train_label.sum()/len(train_label)

"""### Data Preprocessing"""

train_data = train_data.to_numpy()

train_label = train_label.to_numpy().astype('float32')

test_data = test_data.to_numpy()

test_label = test_label.to_numpy().astype('float32')

#train_set = np.array(train_set.as_matrix())
#train_label = np.array(pd.DataFrame(train_label).as_matrix())

print(type(train_data))
print(type(train_label))
print(type(test_data))
print(type(test_label))

print(train_data.dtype)
print(train_label.dtype)
print(test_label.dtype)
print(test_data.dtype)

"""### Model Architecture

- Model : 1 hidden layers including 16 unit.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Define model
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1,  activation='sigmoid'))

model.summary()

"""- Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)"""

from tensorflow.keras import optimizers

model.compile(optimizer = 'RMSprop', loss='binary_crossentropy', metrics=['accuracy'])

"""- Train the Model with Epochs (100)."""

history = model.fit(train_data, train_label, validation_split=0.2, epochs=75, batch_size = 16)

history.history.keys()

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs = range(75)
plt.plot(epochs, loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
plt.plot(epochs, acc_values, 'bo', label='Training acc')
plt.plot(epochs, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""- Evaluation Step
- Prediction
"""

score = model.evaluate(test_data, test_label)

score

"""- If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .
- Prediction should be > **92%**
"""

predictions=model.predict(test_data)

y_pred = (predictions > 0.5)

tf.math.confusion_matrix(
    test_label, y_pred, num_classes=2, weights=None, dtype=tf.dtypes.int32,
    name=None
)

# It will evaluate the logical expression y_predict>0.25 and return True or False

np.count_nonzero(y_pred)