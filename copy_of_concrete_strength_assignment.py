# -*- coding: utf-8 -*-
"""Copy of Concrete Strength Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14-k5HE--DIztHvujgiYm8wgArKkI3M_f

# Assignment: Compresive Strength Concrete Problem


### Abstract: 

Concrete is the most important material in civil engineering. The concrete compressive strength (concrete strength to bear the load) is a highly nonlinear function of age and ingredients.  <br><br>

<table border="1"  cellpadding="6" bordercolor="red">
	<tbody>
        <tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Data Set Characteristics:&nbsp;&nbsp;</b></p></td>
		<td><p class="normal">Multivariate</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Instances:</b></p></td>
		<td><p class="normal">1030</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Area:</b></p></td>
		<td><p class="normal">Physical</p></td>
        </tr>
     </tbody>
    </table>
<table border="1" cellpadding="6">
    <tbody>
        <tr>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Attribute Characteristics:</b></p></td>
            <td><p class="normal">Real</p></td>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Number of Attributes:</b></p></td>
            <td><p class="normal">9</p></td>
            <td bgcolor="#DDEEFF"><p class="normal"><b>Date Donated</b></p></td>
            <td><p class="normal">2007-08-03</p></td>
        </tr>
     </tbody>
    </table>
<table border="1" cellpadding="6">	
    <tbody>
    <tr>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Associated Tasks:</b></p></td>
		<td><p class="normal">Regression</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Missing Values?</b></p></td>
		<td><p class="normal">N/A</p></td>
		<td bgcolor="#DDEEFF"><p class="normal"><b>Number of Web Hits:</b></p></td>
		<td><p class="normal">231464</p></td>
	</tr>
    </tbody>
    </table>

###  Description:
| Features Name | Data Type | Measurement | Description |
| -- | -- | -- | -- |
Cement (component 1) | quantitative | kg in a m3 mixture | Input Variable
Blast Furnace Slag (component 2) | quantitative | kg in a m3 mixture | Input Variable
Fly Ash (component 3) | quantitative | kg in a m3 mixture | Input Variable
Water (component 4) | quantitative | kg in a m3 mixture | Input Variable
Superplasticizer (component 5) | quantitative | kg in a m3 mixture | Input Variable
Coarse Aggregate (component 6) | quantitative | kg in a m3 mixture | Input Variable
Fine Aggregate (component 7) | quantitative | kg in a m3 mixture | Input Variable
Age | quantitative | Day (1~365) | Input Variable
Concrete compressive strength | quantitative | MPa | Output Variable

### WORKFLOW :
- Load Data
- Check Missing Values ( If Exist ; Fill each record with mean of its feature )
- Standardized the Input Variables. **Hint**: Centeralized the data
- Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).
- Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).
- Compilation Step (Note : Its a Regression problem , select loss , metrics according to it)
- Train the Model with Epochs (100) and validate it
- If the model gets overfit tune your model by changing the units , No. of layers , activation function , epochs , add dropout layer or add Regularizer according to the need .
- Evaluation Step
- Prediction

# Load Data:
[Click Here to Download DataSet](https://github.com/ramsha275/ML_Datasets/blob/main/compresive_strength_concrete.csv)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from keras import models
from keras import layers

from google.colab import drive
drive.mount('/content/drive')

concrete_data = pd.read_csv('/content/drive/MyDrive/AI_Assignment/AI_ASSIGNMENTS/compresive_strength_concrete.txt')

concrete_data.head()



"""Streamlining column names"""

req_col_names = ["Cement", "BlastFurnaceSlag", "FlyAsh", "Water", "Superplasticizer",
                 "CoarseAggregate", "FineAggregate", "Age", "CC_Strength"]
curr_col_names = list(concrete_data.columns)
curr_col_names
mapper = {}
for i,name in enumerate(curr_col_names):
    mapper[name] = req_col_names[i]
  
    
concrete_data = concrete_data.rename(columns = mapper)

concrete_data.head()

concrete_data.isna().sum()

concrete_data.describe()



"""Checking the pairwise relations of Features."""

sns.pairplot(concrete_data)
plt.show()

corr = concrete_data.corr()

plt.figure(figsize=(9,7))
sns.heatmap(corr, annot=True, cmap='Blues')
b, t = plt.ylim()
plt.ylim(b+0.5, t-0.5)
plt.title("Feature Correlation Heatmap")
plt.show()



"""### Observations
* There are'nt any **high** correlations between **Compressive strength** and other features except for **Cement**, which should be the case for more strength.
* **Age** and **Super plasticizer** are the other two features which are strongly correlated with **Compressive Strength**.
* **Super Plasticizer** seems to have a negative high correlation with **Water**, positive correlations with **Fly ash** and **Fine aggregate**.

We can further analyze these correlations visually by plotting these relations.
"""



concrete_data.columns

ax = sns.distplot(concrete_data.CC_Strength)
ax.set_title("Compressive Strength Distribution")

fig, ax = plt.subplots(figsize=(10,7))
sns.scatterplot(y="CC_Strength", x="Cement", hue="Water", size="Age", data=concrete_data, ax=ax, sizes=(50, 300))
ax.set_title("CC Strength vs (Cement, Age, Water)")
ax.legend(loc="upper left", bbox_to_anchor=(1,1))
plt.show()



"""#### Observations from Strength vs (Cement, Age, Water)
* Compressive **strength increases with amount of cement**
* Compressive **strength increases with age**
* Cement with **low age** requires **more cement** for **higher strength**
* The **older the cement** is the **more water** it requires
* Concrete **strength increases** when **less water** is used in preparing it
"""

fig, ax = plt.subplots(figsize=(10,7))
sns.scatterplot(y="CC_Strength", x="FineAggregate", hue="FlyAsh", size="Superplasticizer", 
                data=concrete_data, ax=ax, sizes=(50, 300))
ax.set_title("CC Strength vs (Fine aggregate, Super Plasticizer, FlyAsh)")
ax.legend(loc="upper left", bbox_to_anchor=(1,1))
plt.show()

"""##### Observations from CC Strength vs (Fine aggregate, Super Plasticizer, FlyAsh)
* As **Flyash increases** the **strength decreases**
* **Strength increases** with **Super plasticizer**
"""

fig, ax = plt.subplots(figsize=(10,7))
sns.scatterplot(y="CC_Strength", x="FineAggregate", hue="Water", size="Superplasticizer", 
                data=concrete_data, ax=ax, sizes=(50, 300))
ax.set_title("CC Strength vs (Fine aggregate, Super Plasticizer, Water)")
ax.legend(loc="upper left", bbox_to_anchor=(1,1))
plt.show()

"""##### Observations from CC Strength vs (Fine aggregate, Super Plasticizer, Water)
* **Strength decreases** with **increase in water**, **strength increases** with **increase in Super plasticizer** (already from above plots)
* **More Fine aggregate** is used when **less water**, **more Super plasticizer** is used.

#### Although we are making conclusions by observing the scatter plots, there is an underlying non linear interaction between features which we cannot visualize.
We can visually understand 2D, 3D and max upto 4D plots (by 4D I mean color and size represented by features) as shown above, we can further use row wise and column wise plotting features by seaborn to do further analysis, but still we lack the ability to track all these correlations by ourselves. For this reason, we can turn to Machine Learning to capture these relations and give better insights into the problem.

Fom here we will start processing the data and feed it to machine learning models to correctly predict the Compressive Strength of Concrete given the input features.

# Data Preprocessing
"""

mean_label = concrete_data.iloc[:,-1].mean()
std_label = concrete_data.iloc[:,-1].std()
print(mean_label)
print(std_label)

min_d = concrete_data.min()
max_d = concrete_data.max()
diff = max_d - min_d
normalized_df=(concrete_data - min_d)/ diff
normalized_df.head()

test_d = normalized_df.sample(frac=0.3, random_state=1337)

train_d = normalized_df.drop(test_d.index)

# Commented out IPython magic to ensure Python compatibility.
print(
    "Using %d samples for training and %d for validation"
#     % (len(train_d), len(test_d))
)

train_x = train_d.iloc[:,:-1]
train_y = train_d.iloc[:,-1]

test_x = test_d.iloc[:,:-1]
test_y = test_d.iloc[:,-1]

print(train_x.shape)
print(train_y.shape)

print(test_x.shape)
print(test_y.shape)

train_x.describe()

# train_data = np.array(train_x.iloc[:])
# test_data = np.array(test_x.iloc[:])

train_data = train_x.to_numpy()
test_data = test_x.to_numpy()

test_labels= np.array(test_y.astype('float32'))
train_labels= np.array(train_y.astype('float32'))

# train_labels= np.array(train_y.astype('float32'))

train_labels.shape

# we are passing activation function as a parameter here so that we can call this function with tanh or relu while
# fitting and training the model

def build_model(act):
  model = models.Sequential()
  model.add(layers.Dense(10, activation= act,input_shape=(train_data.shape[1],)))
  model.add(layers.Dense(8, activation= act))
  model.add(layers.Dense(6, activation= act))
  model.add(layers.Dense(1))
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
  return model

build_model('relu').summary()

model_relu = build_model('relu')
history = model_relu.fit(train_data, train_labels,epochs= 100, batch_size=1, validation_split=0.3)

test_mse_score, test_mae_score = model_relu.evaluate(test_data, test_labels)

history.history.keys()

val_mae = history.history['val_mae']
val_loss = history.history['val_loss']

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(len(history_dict['loss']))
plt.plot(epochs, loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc_values = history_dict['mae']
val_acc_values = history_dict['val_mae']
plt.plot(epochs, acc_values, 'bo', label='Training mae')
plt.plot(epochs, val_acc_values, 'b', label='Validation mae')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('mae')
plt.legend()
plt.show()

y_predict = model_relu.predict(test_data)
y_predict.shape

def denorm(x):
    return (x * std_label) + mean_label

y_predict = denorm(y_predict)
y_predict = y_predict.flatten()
y_predict.shape

y_true = denorm(test_labels)
y_true = y_true.flatten()
test_labels.shape

coef = np.polyfit(y_true,y_predict,1)
poly1d_fn = np.poly1d(coef) 
# poly1d_fn is now a function which takes in x and returns an estimate for y

plt.plot(y_true, y_predict, 'bo', y_true, poly1d_fn(y_true), '--k')
plt.xlim(0, 60)
plt.ylim(0, 60)



